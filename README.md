# KNN-Classifier

python code: https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/iris%20KNN%20(v01).py
dataset: https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/Iris.csv

## 01 - PROJECT & DATA
While there are several other Machine Learning techniques capable of solving much more complex problems and tackling grater dimensions of data and number of parameters involved, choosing some of them for this specific task could be considered an overkill. The use of AI must follow a sustainable proposition and justify the amount of time and resources dedicated to its implementation. The KNN model bears plenty of capability to handle the classification of the Iris Flower dataset. 

### Project flowchart:
![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20flowchart.png)

Also known as Fisher’s dataset (Fisher, R., 1936) the Iris Flower set comprehends 150 instances of data containing selected characteristics of three sub-species of the Iris Flower (Vatshayan, S., 2019). It contains individual measurements for each specimen describing their respective petal length and width as well as sepal length and width. There are 50 instances of each species, iris-setosa, iris-virginica and iris-versicolor, and the pattern generated by similar measurements on each of the different species allow for the algorithm to identify unseen examples.
  
![img_ Iris Species](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20Iris%20Species.png)

## 02 - APPROACH & IMPLEMENTATION DETAILS 
	
The algorithm used in this Project was programmed in python. PyCharm was chosen as the programming environment.
Libraries used: 

-> Numpy: allows for operations with multi-dimensional arrays of data
-> Pandas: process and analyse data in various formats
-> Seaborn: generates the heat-map 
-> Matplotlib: plots data into graphs 
-> scikit-learn: library dedicated to machine learning algorithms 

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20libraries.png)

	  
Import and prepare the data:

Remove the index labels and separate it into a variable x with the parameter values and variable y with the classes. Encode the class variable into numerical values 0-setosa, 1-versicolor, and 3-virginica and fit them accordingly, the x instances with their respective y class with sklearn’s functions LabelEncoder & fit_transform.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20prepare%20data.png)

Once variables are defined and encoded, the data must be randomized and divided into training and test sets. This process can be achieved through the train_test_split function. Initially, I arbitrarily configured the data split to 20% test and 80% training, and experimented variations to verify changes in performance.
 
![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20split%20train%20test.png)

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20data%20sample.png)

The optimal number of k-neighbours will vary according to the problem and data in hand, being advisable to set the k value to account for more than 02 neighbours. We started with an arbitrary value of 5 and experimented variations. It is also necessary to define the metric to calculate the distances. The chosen metric for this model is the Euclidean distance, which on scikit-learn’s library is defined by the variation p=2 of the Minowski formula. It derives from the Pythagoras theorem and can be applied on our case of 2-dimentional plans. For 3D coordinates, the z difference to the power of 2 must be added to the sum of square roots. The weight assigned to each instance was kept the default configuration uniform, the other options would be distance, where each point is weighted by the inverse of their distance or a customised function.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20euclidean.png)

Ploting matrices:

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20matrix%20plot.png)

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20matrix%20plot.png)

## 03 - PERFORMANCE EVALUATION

While 96.67% accuracy achieved with the initial configuration, it is always advisable to test variations on the algorithm parameters, aiming to reach optimal performance. Were tested 3 variations in test sample size, 20%, 30% and 40%, and 4 variations in the number of K-Neighbours considered, 2, 3, 5 and 10. The highest accuracy score of 100% was achieved with 10-Nearest Neighbours and test sample corresponding to 20% of the total instances of data available, while 93.33%, the lowest, was obtained considering only 2 neighbour instances and the same 20% training/test split, which was the data allocation more consistent across the variations of k values.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20performance.png)


