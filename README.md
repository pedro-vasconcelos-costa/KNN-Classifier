# KNN-Classifier

python code: https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/iris%20KNN%20(v01).py

dataset: https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/Iris.csv

## 01 - PROJECT & DATA
While there are several other Machine Learning techniques capable of solving much more complex problems and tackling grater dimensions of data and number of parameters involved, choosing some of them for this specific task could be considered an overkill. The use of AI must follow a sustainable proposition and justify the amount of time and resources dedicated to its implementation. The KNN model bears plenty of capability to handle the classification of the Iris Flower dataset. 

### Project flowchart:
![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20flowchart.png)

Also known as Fisher’s dataset (Fisher, R., 1936) the Iris Flower set comprehends 150 instances of data containing selected characteristics of three sub-species of the Iris Flower (Vatshayan, S., 2019). It contains individual measurements for each specimen describing their respective petal length and width as well as sepal length and width. There are 50 instances of each species, iris-setosa, iris-virginica and iris-versicolor, and the pattern generated by similar measurements on each of the different species allow for the algorithm to identify unseen examples.
  
![img_ Iris Species](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20Iris%20Species.png)

## 02 - APPROACH & IMPLEMENTATION DETAILS 
	
The algorithm used in this Project was programmed in python. PyCharm was chosen as the programming environment.
Libraries used: 

-> Numpy: allows for operations with multi-dimensional arrays of data
-> Pandas: process and analyse data in various formats
-> Seaborn: generates the heat-map 
-> Matplotlib: plots data into graphs 
-> scikit-learn: library dedicated to machine learning algorithms 

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20libraries.png)

	  
Import and prepare the data:

Remove the index labels and separate it into a variable x with the parameter values and variable y with the classes. Encode the class variable into numerical values 0-setosa, 1-versicolor, and 3-virginica and fit them accordingly, the x instances with their respective y class with sklearn’s functions LabelEncoder & fit_transform.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20prepare%20data.png)

Once variables are defined and encoded, the data must be randomized and divided into training and test sets. This process can be achieved through the train_test_split function. Initially, I arbitrarily configured the data split to 20% test and 80% training, and experimented variations to verify changes in performance.
 
![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20split%20train%20test.png)

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20data%20sample.png)

The optimal number of k-neighbours will vary according to the problem and data in hand, being advisable to set the k value to account for more than 02 neighbours. We started with an arbitrary value of 5 and experimented variations. It is also necessary to define the metric to calculate the distances. The chosen metric for this model is the Euclidean distance, which on scikit-learn’s library is defined by the variation p=2 of the Minowski formula. It derives from the Pythagoras theorem and can be applied on our case of 2-dimentional plans. For 3D coordinates, the z difference to the power of 2 must be added to the sum of square roots. The weight assigned to each instance was kept the default configuration uniform, the other options would be distance, where each point is weighted by the inverse of their distance or a customised function.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20euclidean.png)

### Ploting confusion matrices:

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20matrix%20plot.png)

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/k5%20test%203.png)

## 03 - PERFORMANCE EVALUATION

While 96.67% accuracy achieved with the initial configuration, it is always advisable to test variations on the algorithm parameters, aiming to reach optimal performance. Were tested 3 variations in test sample size, 20%, 30% and 40%, and 4 variations in the number of K-Neighbours considered, 2, 3, 5 and 10. The highest accuracy score of 100% was achieved with 10-Nearest Neighbours and test sample corresponding to 20% of the total instances of data available, while 93.33%, the lowest, was obtained considering only 2 neighbour instances and the same 20% training/test split, which was the data allocation more consistent across the variations of k values.

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20performance.png)

It is important to note that we are dealing with a relatively small amount of data and the differences in accuracy levels could be enlarged when working with larger and more complex datasets, as well as the 100% achieved by the model for the iris set, may not maintain its 0-error performance if applied to other projects.![image](https://user-images.githubusercontent.com/130906484/232328418-c55df557-b872-40a3-b864-c8dd5c41a251.png)

### Comparison to other researched approaches:

![image](https://github.com/pedro-vasconcelos-costa/KNN-Classifier/blob/main/img_%20accuracy.png)

# 04 - CHALLENGES & CONCLUSION
Throughout the making of this project, it was possible to identify that K-Nearest Neighbour algorithms are indeed of a simple construct, but often simple problems ask for simple and efficient solutions. It also brings some advantages, parameters are relatively easy to modify and it allows for thorough understanding of the algorithm’s functionalities and nuances of the project. There is plenty of documentation and research on the topic, tools and compatibility with developing environments. We were able to experiment changes in training/test splits and number of k-neighbours to consider, isolating the effects of each manipulation and extracting valuable insights from the problem as it was demonstrated in this report.

One of the biggest challenges was the limited size and complexity of the data. Running a test sample of 30 instances makes for a single error to represent a great variation in performance, what could be misleading. Given the small number of incorrect classifications across all variations applied, we can speculate that the dataset only contained a few instances of data to actually challenge the efficiency of the model. At the same time the iris-flower classification problem was tackled by a wide range of different models and techniques, which allows for a good amount of information for comparison between those systems and when contrasting the KNN with them we can state that it can produce satisfactory results. Secondly, the simplicity of construction, demanded computational power and running time make possible integrations with other techniques, producing hybrid systems, what represents a great opportunity for improvement. 


